---
title: "R Notebook"
output: html_notebook
---

```{r}
library(dplyr)
library(tidyverse)
library(tsibble)
library(fpp3)
library(tidymodels)
source('etl.R')
original_train <- get_train()
enriched_train <- 
  original_train|>
  left_join_gdp()
  
original_test <- get_test()
ts_keys=c('country','store','product')
ts_index='date'
```

# Forecasting with Linear Regression

base on the awesome kaggle contribution link: <https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline#Forecasting-with-Linear-Regression>

## Contents

## 1 EDA - A brief EDA, showing the essentials

missing values

```{r}
#| fig.height: 9
#| fig.width: 6
gap_ts_name_df <- original_train |> 
  index_by(floor_date(date,'10 year'))|>
  group_by_key() |>
  summarize(total_na=sum(is.na(num_sold))) |> 
  filter(total_na>0)|>
  arrange(total_na)|>
  as_tibble()|>
  select(1,2,3)
missing_ts <- gap_ts_name_df |> 
  left_join(original_train,
            by=c('country','store','product'))|>
  mutate(ts_id = paste(country,store,product,sep='-'))#|>
  #as_tsibble(key=ts_keys)
missing_ts|>
  ggplot(aes(x=date, y= num_sold))+
  geom_line(color='blue')+
  geom_vline(data = .%>%filter(is.na(num_sold)) ,aes(xintercept = date),color='red',alpha=0.1)+
 # # geom_point(aes(x=date,y=is.na(num_sold)))+
   facet_wrap(vars(ts_id),ncol=1,scales='free_y')+
   theme( legend.position = "off",
          panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())+
  theme_minimal()

```

The missing data is not missing completely randomly (with respect to time), some periods contain more missing data than others.

It looks data is missing when the value for `num_sold < 200` for `Canada` and `< 5 for Kenya` (for some of the time series).

The training data starts on `r min(original_train$date)` and ends on `r max(original_train$date)`



We have 7 years of data to train on occuring at daily frequency.

We are required to forecast 3 years.

skip. pls refer to the [originallink](https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline#Forecasting-with-Linear-Regression)

## 2. Aggregating Categorical Variables

The main theme of this notebook is to show that its a good idea to aggregate the time series across each of the three categorical variables: Store, Country and Product. 

### 2.1 Country
#### train
First we show that its a good idea to aggregate countries when we make the forecast.

To do this we need to show that the proportion of total sales for each country remains constant, regardless of time.

In the graph below, we are looking for straight lines for each country:

```{r}
country_ratio <- 
  enriched_train|>
  as_tibble()|>
  mutate(total_sold_daily=sum(num_sold,na.rm=T),.by = date)|>
  mutate(num_sold_propotion=sum(num_sold/total_sold_daily,na.rm=T),
            .by=c(country,date))
country_ratio|>
  ggplot(aes(x=date, y=num_sold_propotion,color=country)) +
  geom_line()+
  geom_line(aes(x=date, y=gdp,group=country),linewidth = 1)+
  labs(title = "Proportion of Stickers Sold Daily VS Country",
       subtitle= 'gdp ratio reference')+
  theme_minimal()
  
```

The lines are not perflectly straight, meaning a single constant does not explain the proportion of sales regardless of time.

The lines for each country do seem to have rises and falls each year (noteably exactly at the year markings) something artificially strange is going on here.

The link seems to be GDP per captia.

#### test with gdp
```{r}
get_gdp()|>mutate(date=paste(year,'-01-01')|>ymd())|> glimpse()

get_gdp()|>
  mutate(date=paste(year,'-01-01')|>ymd())|>
  ggplot(aes(x=date, y= gdp,color=country) )+
  geom_step(,linewidth =1) +
  geom_line(data=country_ratio, aes(x=date, y=num_sold_propotion))
```



-   A continuation of the EDA, showing that we should be able to forecast the aggregated time series (daily total sales) and then disaggregate the forecasts based on historical proportions and other data without penalising performance. 

### 2.2 Store

```{r}

store_ratio <- enriched_train |>
  as_tibble()|>
  mutate(total_sold_daily=sum(num_sold,na.rm=T),.by = date)|>
  summarize(num_sold_propotion=sum(num_sold/total_sold_daily,na.rm=T),
            .by=c(store,date))|>
  mutate(store_propotion_mean = mean(`num_sold_propotion`), .by = store)
# 

store_ratio|>
  ggplot(aes(x=date, y=num_sold_propotion,color=store)) +
  geom_line()+
  geom_line(aes(y=store_propotion_mean,group=store),,linewidth = 1.5)+
  labs(title = "Proportion of Stickers Sold Daily VS store" )+
  theme_minimal()

```
The store based num_sold ratios remain constant. This means we can generally predict the proportion of sales for each store, regardless of when it occurs.



### 2.3 Product
```{r}

product_ratio <-
  enriched_train |>
  as_tibble()|>
  mutate(total_sold_daily=sum(num_sold,na.rm=T),.by = date)|>
  mutate(num_sold_propotion=sum(num_sold/total_sold_daily,na.rm=T),
            .by=c(product,date))|>
  mutate(product_propotion_mean = mean(`num_sold_propotion`), .by = c(date,product))

  
product_ratio|>
  ggplot(aes(x=date, y=num_sold_propotion,color=product)) +
  geom_line()+
  geom_line(aes(y=product_propotion_mean,group=product),,linewidth = 1)+
  labs(title = "Proportion of Stickers Sold Daily VS product" )+
  theme_minimal()


```


- Observations

The product ratio shows clear sinsidual lines for each product, with a period of 2 years(maximum).

- Insight

As we have a clear seasonal pattern of the ratio of sales for each product, we do not need to forecast each product individually (or treat product as a categorical variable etc.). Instead we can forecast the sum of all sales each day, then afterwards convert the forecasted sum down to the forecast for each product, using the forecasted ratios for each date.

- Conclusions

All this together means we only need to forecast 2 time series:

The total sales each day
The ratio in number of sales for each product each day
We still need to be careful about some timeseries where we might not have sales, or missing data.

Once we have completed the forecasts we can break the forecast down into the 3 categorical variables: Product, Country and Store.


### 2.4Time seriess aggreation
#### day, week, month
```{r}

enriched_train|>
  index_by(monthly=lubridate::floor_date(date,'month'))|>
  summarize(total_sold=sum(num_sold,na.rm=T))|>
  autoplot(total_sold,color='blue')+
  labs(title='total saels number per month')+
  theme_minimal()

enriched_train|>
  index_by(monthly=lubridate::floor_date(date,'week'))|>
  summarize(total_sold=sum(num_sold,na.rm=T))|>
  autoplot(total_sold,color='blue')+
  labs(title='total saels number per week')+
  theme_minimal()
enriched_train|>
  index_by()|>
  summarize(total_sold=sum(num_sold,na.rm=T))|>
  autoplot(total_sold,color='blue')+
  labs(title='total saels number per day')+
  theme_minimal()

```
#### seasonal
```{r}
enriched_train|>
 summarize(num_sold=sum(num_sold,na.rm=T))|>
 gg_season(num_sold,period='1w')
 labs(title='weekly period trends')
enriched_train|>
 summarize(num_sold=sum(num_sold,na.rm=T))|>
 gg_season(num_sold,period='1y')+
 labs(title='yearly period trends')
```
## 3. Fill missing value
### total missing 
```{r}
missing_count_df <- enriched_train |>
  group_by_key() |>
  index_by(alltime = floor_date(date,'10 years')) |>
  summarize(missing_count=sum(is.na(num_sold)))|>
  as_tibble()|>
  select(-alltime)|>
  filter(missing_count>0)|>
  arrange(missing_count)

missing_count_df|>arrange(country,desc(missing_count))|>mutate(case_id=row_number())
  
```

### missing caseid 1,2,3 -Canada goose product for all stores( Discount,less Premium )
Goose(2557,1308,380	  missing), fill na by using Norway num_sold * gdp ratio between Canada / Norway
```{r}
# use norway to replace canada missing value 
no_enriched_train <-
  enriched_train|>
  filter(country=='NO')|>
  select(date, source_country=country, store, product,source_num_sold=num_sold, source_gdp=gdp)

fillna_case_123 <-
  enriched_train|>
  filter(country=='CA' & store %in% c('disc','less','prem') & product=='goose' & is.na(num_sold))|>
  left_join(no_enriched_train, by=c('date','store','product'))|>
  mutate(num_sold=case_when(is.na(num_sold)~source_num_sold * (gdp/source_gdp),
                            .default=num_sold))|>
  select(id, date, country,store, product, num_sold, gdp)
fillna_case_123|>
  index_by(all_period=floor_date(date,'10 years'))|>
  summarize(total_missing=sum(is.na(num_sold))) |>pull(2)
```
###  missing caseid 5,6,7,8 Kenya goose & kerneler product 
missing value for Kenya
KE	disc	kern	63	
KE	prem	goose	646	
KE	less	goose	1358	
KE	disc	goose	2557	
Fill na based Norway sold number * gdp ratio bwteen Kenya & Normal.
Note: TODO: Singapore could be try as it is in Hot area as well. but follow best practice first.
```{r}
# use Norway to replace Kenya missing value 
no_enriched_train <-
  enriched_train|>
  filter(country=='NO')|>
  select(date, source_country=country, store, product,source_num_sold=num_sold, source_gdp=gdp)

fillna_case_5678<-
  enriched_train|>
  filter(country=='KE' & store %in% c('disc','less','prem') & product %in% c('goose','kern'),
         is.na(num_sold))|>
  left_join(no_enriched_train, by=c('date','store','product'))|>
  
  mutate(num_sold=case_when(is.na(num_sold)~source_num_sold * (gdp/source_gdp),
                            .default=num_sold))|>
  select(id, date, country,store, product, num_sold, gdp)

fillna_case_5678|>
  index_by(all_period=floor_date(date,'10 years'))|>
  summarize(total_missing=sum(is.na(num_sold))) |>pull(2)
```

### missing caseid, 4,9
CA	disc	kern	1	
KE	disc	kdm	1
manually fill with 4, 195 based on kaggle best practice.
```{r}
fillna_case_4 <-
enriched_train |> 
  filter(country=='CA', store =='disc' , product=='kern', is.na(num_sold))|>
  mutate(num_sold =4)
fillna_case_9 <-
enriched_train |> 
  filter(country=='KE', store =='disc' , product=='kdm', is.na(num_sold))|>
  mutate(num_sold =195)
```

#### bind fillna_case with train data to complete
```{r}
fill_enriched_train <-
  enriched_train|>
  filter(!is.na(num_sold))|>
  bind_rows(fillna_case_9,fillna_case_4,
          fillna_case_5678,
          fillna_case_123)

fill_enriched_train|>
  index_by(all_time=floor_date(date,'10 years'))|>
  summarize(total_missing=sum(is.na(num_sold))) |>pull(total_missing)

```



## 3 Modeling
### 3.1 Total Sales Forecast
```{r}
agged_train <- fill_enriched_train|>
  summarize(num_sold=sum(num_sold))

agged_train |>ggplot(aes(x=date, y= num_sold)) +geom_line(color='blue') 
```
```{r}
agged_test <-original_test |>as_tibble()|>
  group_by(date)|>
  summarize(id=first(id))

# original python code
# TODO to be understand how to aggregation test data with agged traing data
# #get the dates to forecast for
# test_total_sales_df = test_df.groupby(["date"])["id"].first().reset_index().drop(columns="id")
# #keep dates for later
# test_total_sales_dates = test_total_sales_df[["date"]]
```


#### recipes
```{r}
rcp <- 
  recipe(num_sold ~ ., data =agged_train )|> 
  # 提取月份
  step_date(date, features = c("month"))|>

    # 计算月份的三角函数特征
    step_mutate(month_sin = sin(month(date) * (2 * pi / 12)),
                month_cos = cos(month(date) * (2 * pi / 12)))|> 
    # 提取星期几并重新编码
  # TODO way output is different than python code. need double check later
    step_mutate(day_of_week = lubridate::wday(date,week_start =1),
                day_of_week = as_factor(case_when( day_of_week <= 4 ~ 0,
                                         day_of_week == 5 ~ 1,
                                         day_of_week == 6 ~ 2,
                                         day_of_week == 7 ~ 3 ))) |>
    # 提取一年中的第几天（考虑闰年）
    step_mutate(day_of_year = yday(date),
                day_of_year = ifelse(leap_year(date) & month(date) > 2, 
                                     day_of_year - 1, 
                                     day_of_year))|> 
    # 计算一年中第几天的三角函数特征
    step_mutate(day_sin = sin(day_of_year * (2 * pi / 365)),
                day_cos = cos(day_of_year * (2 * pi / 365))) |>
    # 标记重要日期
    step_mutate(important_dates =
                  as_factor(ifelse(day_of_year %in% c(1:10, 99:101, 125:126, 355:365), 
                                   day_of_year,
                                   0)) )|>
    # 删除原始日期列和中间列
    step_rm(date, date_month, day_of_year) |>
    # 对分类变量进行独热编码
    step_dummy(important_dates, day_of_week, one_hot = TRUE)|>
  step_nzv(all_predictors())|>
  
  #step_corr(all_predictors())|>
  check_missing()

rcp|>prep()|>bake(agged_train) |>head()
```
#### check testing data 
```{r}
rcp|>prep()|>bake(agged_test) |>head()
```
### Modeling

```{r}

# Specify the ridge regression model
ridge_model <- linear_reg(penalty = 0.01, mixture = 0) %>%  # mixture = 0 for ridge
  set_engine("glmnet")

workflow <- workflow() %>%
  add_recipe(rcp) %>%
  add_model(ridge_model)
# Train the model
fit <- workflow %>%
  fit(data = agged_train)

# Make predictions on the test data
predictions <- fit %>%
  predict(new_data = agged_train) %>%
  bind_cols(agged_train)


# Evaluate the model performance
metrics <- predictions %>%
  metrics(truth = num_sold, estimate = .pred)|>
  bind_rows(mape(predictions, truth = num_sold, estimate = .pred))

# Print the metrics
print(metrics)

```

```{r}
predictions|>as_tsibble(index=date)|>autoplot(num_sold)
```


-   Forecast the total number of sales across all categorical variables using Linear Regression for 2017, 2018 and 2019.

### 3.2. Product Sales Ratio Forecast

-   Forecast the ratio of sales between products for 2017, 2018 and 2019.

### 4. Dissagregating Total Sales Forecast

-   Disagreggate the Total Sales forecasts, to get the forecast for each categorical variable.

## 5 submssion

. References

This work is based off my notebook from Season 2, on a similar competition:

<https://www.kaggle.com/code/cabaxiom/tps-sep-22-eda-and-linear-regression-baseline>
